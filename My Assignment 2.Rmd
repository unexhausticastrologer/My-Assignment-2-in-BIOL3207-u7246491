---
title: "R Notebook"
output: html_notebook
---


```{r}
library(plyr)
library(tidyverse)
library(ggplot2)
library(flextable)
library(metafor)
library(orchaRd)
```

### 1. Correct analysis of Clark et al. (2020) data (i.e., OA_activitydat_20190302_BIOL3207.csv) to generate the summary statistics (means, SD, N) for each of the fish species’ average activity for each treatment.
```{r}
oa.csv <- read.csv("OA_activitydat_20190302_BIOL3207.csv", stringsAsFactors = TRUE) %>% drop_na()
head(oa.csv)
```

```{r}
st.sum <- ddply(oa.csv, c('treatment', "species"), summarize, n = length(activity), mean = mean(activity), sd = sd(activity))
ft <- flextable(st.sum)
ft
st.sum
```

acantho = Acanthochromis; Ambon = Pomacentrus amboinensis; Chromis = Chromis atripectoralis; Humbug = Dascyllus aruanus; Lemon = Pomacentrus moluccensis
```{r}
st.sum <- st.sum %>% mutate(species=case_when(species == "acantho" ~ "Acanthochromis polyacanthus",
                                              species == "ambon" ~ "Pomacentrus amboinensis",
                                              species == "chromis" ~ "Chromis atripectoralis",
                                              species == "humbug" ~ "Dascyllus aruanus",
                                              species == "lemon" ~ "Pomacentrus moluccensis",
                                              species == "whitedams" ~ "Dischistodus perspicillatus"),
                            treatment=case_when(treatment == "CO2" ~ "oa",
                                                treatment == "control" ~ "ctrl"))
```

```{r}
st.sum
```

```{r}
st.order <- pivot_wider(data = st.sum, names_from = "treatment", values_from = c("n", "mean", "sd") ,names_sep = ".")
st.order
```

```{r}
colnames(st.order) <- c("Species", "oa.n", "ctrl.n", "oa.mean", "ctrl.mean", "oa.sd", "ctrl.sd")
st.order
```

### 2. Through coding, merge the summary statistics generated from 1) with the metadata (i.e., clark_paper_data.csv) from Clark et al. (2020).
```{r}
meta.csv.92 <- read.csv("clark_paper_data.csv", stringsAsFactors = TRUE)
meta.csv.92$Cue.stimulus.type <- "Predator"
glimpse(meta.csv.92)
```

```{r}
meta.92 <- merge(meta.csv.92, st.order)
meta.92
```
### 3. Through coding, correctly merge the combined summary statistics and metadata from Clark et al. (2020) (output from 1 & 2) into the larger meta-analysis dataset (i.e., ocean_meta_data.csv).
```{r}
meta.csv <- read.csv("ocean_meta_data.csv", stringsAsFactors = TRUE)
levels(meta.csv$Species)
```

```{r}
meta.csv$Pub.year.IF <- meta.csv$Pub.year.IF %>% as.numeric()
meta.csv$X2017.IF <- meta.csv$X2017.IF %>% as.numeric()
```

```{r}
meta.all <- bind_rows(meta.csv, meta.92)
glimpse(meta.all)
```

### 4. Correctly calculate the log response ratio (lnRR) effect size for every row of the dataframe using metafor’s escalc() function.
```{r}
meta.lrr <- escalc(measure="ROM", m1i=ctrl.mean, m2i=oa.mean, sd1i=ctrl.sd, sd2i=oa.sd, n1i=ctrl.n, n2i=oa.n, var.names = c("LRR", "LRR_var"), data=meta.all) %>% drop_na() %>% subset(LRR_var < 10) # restrict the size of standard error
```

```{r}
meta.lrr$residual <- 1:dim(meta.lrr)[1]
```

### 5. Correct meta-analytic model fitted to the data that controls for the sampling variance of lnRR. The model should include a random effect of study and observation. Use metafor’s rma.mv() function.

1) Glance the $I^2$
```{r}
meta.FE <- rma(LRR, LRR_var,method = 'FE',data = meta.lrr)
summary(meta.FE)
```

$I^2$ is too high to use FE model, choose REML instead.

multilevel test
```{r}
names(meta.all)
```

```{r}
meta.ml <- rma.mv(LRR, LRR_var, method = "REML", random = list(~1 | Study, ~1 | residual), dfs = "contain", test = "t", data = meta.lrr)
meta.ml
```

### 6. Written paragraph of the findings and what they mean which is supported with a figure. The paragraph should include:
1) Correct presentation and interpretation of overall meta-analytic mean and measures of uncertainty around the mean estimate (e.g., 95% confidence intervals).

For the different studies, the effect size is -0.1300, the standard error is 0.1128, the t-value is -1.1526, the p-val is 0.2521 > 0.05 which means that the $H_0$:the value of LRR are 0 cannot be rejected, the 95% confidence interval is from -0.3541 to 0.0940 which means the LRR value have 95% possibility located in this range. the p-value in the heterogeneity test is less than 0.0001 which means that there is a heterogeneity generally while it cannot be confirmed that which level has the huge heterogeneity.

2) Measures of heterogeneity in effect size estimates across studies (i.e., I2 and/or prediction intervals - see predict() function in metafor)

```{r}
i2_ml(meta.ml, data = meta.lrr)

```

```{r}
pis <- predict(meta.ml, transf = "transf.logit")
pis
```

```{r}
forest(meta.ml)
```

```{r}
orchard_plot(meta.ml, group = "Study" , data = meta.lrr,
    xlab = "log response ratio", angle = 45)
```

```{r}
meta.lt <- rma.mv(LRR~Life.stage, LRR_var, method = "REML", random = list(~1 | Study, ~1 | residual), dfs = "contain", test = "t", data = meta.lrr)
meta.lt
```

```{r}
orchard_plot(meta.lt, group = "Study" ,mod = "Life.stage", data = meta.lrr,
    xlab = "log response ratio", angle = 45)
```

### 7. Funnel plot for visually assessing the possibility of publication bias.
```{r}
funnel(meta.ml.le, level = c(0.1, 0.05, 0.01), shade = c("white", "gray55", "gray 75"),
    las = 1, atransf = tanh, legend = TRUE)
```

```{r}
ggplot(meta.lrr, aes(y = LRR, x = LRR_var)) + geom_point() + geom_smooth(method = lm)
```
It seems that there is no bias in these study.

### 8. Time-lag plot assessing how effect sizes may or may not have changed through time.
```{r}
ggplot(meta.lrr, aes(y = LRR, x = Year..online., size = 1/sqrt(LRR_var))) + geom_point(alpha = 0.3) +
    geom_smooth(method = lm, col = "red", show.legend = FALSE) + labs(x = "Publication Year",
    y = "Log response ratio (LRR)", size = "Precision (1/SE)") +
    theme_classic()
```
It seems that there is a little time bia in this meta analysis.

### 9.Formal meta-regression model that includes year as a moderator (fixed effect) to test for time-lag bias
```{r}
meta.ml.year <- rma.mv(LRR ~ Year..online., LRR_var, method = "REML", random = list(~1 | Study, ~1 | residual), dfs = "contain", test = "t", data = meta.lrr)
summary(meta.ml.year)
```
### 10. Formal meta-regression model that includes inverse sampling variance (i.e., 1vlnRR) to test for file-drawer biases
```{r}
meta.ml.r <- rma.mv(LRR ~ 1/LRR_var, LRR_var, method = "REML", random = list(~1 | Study, ~1 | residual), dfs = "contain", test = "t", data = meta.lrr)
summary(meta.ml.r)
```

### 11. A written paragraph that discusses the potential for publication bias based on the meta-regression results. What type of publication bias, if any, appears to be present in the data? If publication bias is present, what does it mean and what might be contributing to such bias?

First of all, the result of funnel plot shows that the symmetric of itself are very good, which means that there would be not publish bias in this meta analysis. After several regression test, the results of heterogeneity are all quite large and the corresponding p-values are all smaller than 0.0001, which means that the bias are not in the tested factors. The result of following test evidence the result of the funnel plot.

### 12. Identify any studies contributing to publication bias. How do your updated meta-analysis results compare with a meta-analysis by Clement et. al. (2022)? Are there any concerns about these studies? If so, describe using references to existing papers what concerns have been raised?

```{r}
meta.rm <- escalc(measure="ROM", m1i=ctrl.mean, m2i=oa.mean, sd1i=ctrl.sd, sd2i=oa.sd, n1i=ctrl.n, n2i=oa.n, var.names = c("LRR", "LRR_var"), data=meta.all) %>% drop_na() %>% subset(LRR_var > 10)
```

```{r}
meta.et <- rma.mv(LRR ~ Effect.type, LRR_var, method = "REML", random = list(~1 | Study, ~1 | residual), dfs = "contain", test = "t", data = meta.lrr)
summary(meta.et)
```

There are several observations in some studies have a large st.error. It seems that the Clement et. al. (2022) study did not filtered them. And there were several bias in the effect type in this study while in my test it has no bias in this factor after filtered the data. the further reading for the observations with large st.error is necessary to figure out the difference.


